{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpar5LziY_-0"
      },
      "source": [
        "#Zadanie 4 (7 pkt)\n",
        "Celem zadania jest zaimplementowanie algorytmu drzewa decyzyjnego ID3 dla zadania klasyfikacji. Trening i test należy przeprowadzić dla zbioru Iris. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania drzewa dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
        "\n",
        "* Implementacja funkcji entropii - **0.5 pkt**\n",
        "* Implementacja funkcji entropii zbioru - **0.5 pkt**\n",
        "* Implementacja funkcji information gain - **0.5 pkt**\n",
        "* Zbudowanie poprawnie działającego drzewa klasyfikacyjnego i przetestowanie go na wspomnianym wcześniej zbiorze testowym. Jeśli w liściu występuje kilka różnych klas, decyzją jest klasa większościowa. Policzenie accuracy i wypisanie parami klasy rzeczywistej i predykcji. - **4 pkt**\n",
        "* Przeprowadzenie eksperymentów dla różnych głębokości drzew i podziałów zbioru treningowego i testowego (zmiana wartości argumentu test_size oraz usunięcie random_state). W tym przypadku dla każdego eksperymentu należy wykonać kilka uruchomień programu i wypisać dla każdego uruchomienia accuracy. - **1.5 pkt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XNc-O3npA-J9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fBh2tfQ44u5k"
      },
      "outputs": [],
      "source": [
        "# funkcja entropii\n",
        "def entropy_func(class_count, num_samples:list):\n",
        "    total_num_samples = sum(num_samples)\n",
        "    entropy = sum([-num_samples[count]/total_num_samples* math.log(num_samples[count]/total_num_samples, 2) if count else 0 for count in range(class_count)])\n",
        "    return entropy\n",
        "\n",
        "\n",
        "# zbiór klas, jako parametr przyjmuje listę klas, zwraca jej długość i entropię zbioru\n",
        "class Group:\n",
        "    def __init__(self, group_classes:list):\n",
        "        self.group_classes = group_classes\n",
        "        self.entropy = self.group_entropy()\n",
        "        self.size = len(group_classes) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.group_classes)\n",
        "\n",
        " # oblicza entropię na podstawie liczby klas, liczby unikalnych klas i tego ile razy wystąpiły\n",
        "    def group_entropy(self):\n",
        "        unique_classes = list(np.unique(self.group_classes))\n",
        "        class_count = len(unique_classes)\n",
        "        num_samples = []\n",
        "        for x in unique_classes:\n",
        "            samples = np.count_nonzero(self.group_classes == x)\n",
        "            num_samples.append(samples)\n",
        "        return entropy_func(class_count, num_samples)\n",
        "\n",
        "# węzeł, przyjmuje indeks atrybutu, wartość względem której następuje podział na lewą i prawą klasę, głębokość w drzewie, węzły dzieci (lewego i prawego) i wartość liścia\n",
        "class Node:\n",
        "    def __init__(self, split_feature, split_val, depth=None, child_node_a=None, child_node_b=None, val=None):\n",
        "        self.split_feature = split_feature\n",
        "        self.split_val = split_val\n",
        "        self.depth = depth\n",
        "        self.child_node_a = child_node_a\n",
        "        self.child_node_b = child_node_b\n",
        "        self.val = val\n",
        "\n",
        "    # zwraca watość decyzyjną danego przykładu\n",
        "    def predict(self, data):\n",
        "        if self.val is None:\n",
        "            if data[self.split_feature] <= self.split_val:\n",
        "                return self.child_node_a.predict(data)\n",
        "            else:\n",
        "                return self.child_node_b.predict(data)\n",
        "        else:\n",
        "            return self.val\n",
        "# drzewo decyzyjne, jako parametry przyjmuje maksymalną głębokość, tworzone na podstawie pierwszego węzła stanowiącego atrybut tree\n",
        "class DecisionTreeClassifier(object):\n",
        "    def __init__(self, max_depth):\n",
        "        self.depth = 0\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    # zwraca entropię każdego ze zbiorów\n",
        "    @staticmethod\n",
        "    def get_split_entropy(group_a, group_b):\n",
        "        split_entropy_a = group_a.entropy\n",
        "        split_entropy_b = group_b.entropy\n",
        "        return split_entropy_a, split_entropy_b\n",
        "         \n",
        "    # informacja, którą zyskujemy dzięki klasyfikacji rodzica na dwójkę dzieci \n",
        "    def get_information_gain(self, parent_group, child_group_a, child_group_b):\n",
        "        parent_entropy = parent_group.entropy\n",
        "        child_a_entropy, child_b_entropy = self.get_split_entropy(child_group_a, child_group_b)\n",
        "        information_gain = parent_entropy - child_a_entropy*(child_group_a.size/parent_group.size) -child_b_entropy*(child_group_b.size/parent_group.size)\n",
        "        return information_gain\n",
        "\n",
        "    # dzieli na lewą (a) i prawą (b) klasę względem jakieś wartości granicznej podziału, dzieli dane według tej wartości\n",
        "    # zwraca obiekty rodzica oraz lewego i prawego dziecka wraz danymi\n",
        "    def get_left_right_split(self, data, feature_values, split_value, classes):\n",
        "        a_classes = [classes[n] for n in range(len(classes)) if feature_values[n] <= split_value]\n",
        "        b_classes = [classes[n] for n in range(len(classes)) if feature_values[n] > split_value]\n",
        "        a_data = [data[n] for n in range(len(classes)) if feature_values[n] <= split_value]\n",
        "        b_data = [data[n] for n in range(len(classes)) if feature_values[n] > split_value]\n",
        "        parent_group = Group(classes)\n",
        "        a_group = Group(a_classes)\n",
        "        b_group = Group(b_classes)\n",
        "        info_gain = self.get_information_gain(parent_group, a_group, b_group)\n",
        "        return info_gain, a_classes, b_classes, a_data, b_data\n",
        "\n",
        "    # najlepszy atrybut decyzyjny, zwraca uzyskaną informację, wartość atrybutu, klasy lewą i prawą wraz z danymi do nich należącymi\n",
        "    def get_best_feature_split(self, data, feature_values, classes):\n",
        "        best_info_gain = 0\n",
        "        best_split_value = None\n",
        "        best_a_classes = None\n",
        "        best_b_classes = None\n",
        "        best_a_data = None\n",
        "        best_b_data = None\n",
        "        for split_value in feature_values:\n",
        "            info_gain, a_classes, b_classes, a_data, b_data = self.get_left_right_split(data, feature_values, split_value, classes)\n",
        "            if info_gain > best_info_gain:\n",
        "                best_info_gain = info_gain\n",
        "                best_split_value = split_value\n",
        "                best_a_classes = a_classes\n",
        "                best_b_classes = b_classes\n",
        "                best_a_data = a_data\n",
        "                best_b_data = b_data\n",
        "                return best_info_gain, best_split_value, best_a_classes, best_b_classes, best_a_data, best_b_data\n",
        "\n",
        "    # najlepszy podział pod względem wszystkich atrybutów i ich wartości, zwraca zyskaną informację, najlepszy atrybut, jego wartość, klasy i dane lewej oraz prawej strony\n",
        "    def get_best_split(self, data, classes):\n",
        "        converted_data = [None]*len(data[0])\n",
        "        for x in range(len(data[0])):\n",
        "          feature_values = [data[i][x] for i in range(len(data))]\n",
        "          converted_data[x] = feature_values\n",
        "        best_info_gain = 0\n",
        "        best_split_feature =  None\n",
        "        best_split_value =  None\n",
        "        best_a_classes = None\n",
        "        best_b_classes = None\n",
        "        best_a_data = None\n",
        "        best_b_data = None\n",
        "        feature_index = 0\n",
        "        for feature_vals in converted_data:\n",
        "            info_gain, split_value, a_classes, b_classes, a_data, b_data = self.get_best_feature_split(data, feature_vals, classes)\n",
        "            if info_gain > best_info_gain:\n",
        "                best_info_gain = info_gain\n",
        "                best_split_value = split_value\n",
        "                best_split_feature = feature_index\n",
        "                best_a_classes = a_classes\n",
        "                best_b_classes = b_classes\n",
        "                best_a_data = a_data\n",
        "                best_b_data = b_data\n",
        "            feature_index += 1\n",
        "        return best_info_gain, best_split_value, best_split_feature, best_a_classes, best_b_classes, best_a_data, best_b_data\n",
        "\n",
        "# buduje drzewo decyzyjne, przyjmuje jako parametry dane, zbiór klas i głębokość, zwraca węzeł lub liść\n",
        "    def build_tree(self, data, classes, depth=0):\n",
        "        if depth == self.max_depth or len(set(classes)) == 1:\n",
        "          samples = dict(Counter(classes))\n",
        "          leaf_value = max(samples, key=samples.get) # jeśli w liściu występuje kilka klas, decyzją jest klasa większościowa \n",
        "          print(\"NEW LEAF\")\n",
        "          print(\"LEAF CLASS VALUE\")\n",
        "          print(leaf_value)\n",
        "          print(\"\\n\")\n",
        "          return Node(None, None, None, None, None, leaf_value)\n",
        "        else:\n",
        "            info_gain, split_value, split_feature, a_classes, b_classes, a_data, b_data = self.get_best_split(data, classes)\n",
        "            if info_gain > 0:\n",
        "                a_branch = self.build_tree(a_data, a_classes, depth + 1)\n",
        "                b_branch = self.build_tree(b_data, b_classes, depth + 1)\n",
        "            print(\"NEW DECISION NODE\")\n",
        "            print(\"INFORMATION GAIN\")\n",
        "            print(info_gain)\n",
        "            print(\"DECISION PARAMETER INDEX\")\n",
        "            print(split_feature)\n",
        "            print(\"LIMIT VALUE\")\n",
        "            print(split_value)\n",
        "            print(\"\\n\")\n",
        "            return Node(split_feature, split_value, depth, a_branch, b_branch)\n",
        "\n",
        "\n",
        "    def predict(self, data):\n",
        "        return self.tree.predict(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "U033RY1_YS8x"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(depth):\n",
        "    dc = DecisionTreeClassifier(depth)\n",
        "    dc.tree = dc.build_tree(x_train, y_train)\n",
        "\n",
        "    predictions = []\n",
        "    for sample, gt in zip(x_test, y_test):\n",
        "        prediction = dc.predict(sample)\n",
        "        predictions.append(prediction)\n",
        "    total_number = len(predictions)\n",
        "    error = 0 \n",
        "    for y_predicted, y in zip(predictions, y_test):\n",
        "        if y_predicted != y:\n",
        "            error += 1\n",
        "    accuracy = (total_number-error)/total_number\n",
        "\n",
        "    print(\"PARAMETERS\")\n",
        "    print(x_test)\n",
        "    print(\"\\n\")\n",
        "    print(\"PREDICTIONS\" \"|\" \"CLASS\")\n",
        "    for y_predicted, y in zip(predictions, y_test):\n",
        "            print(y_predicted , \"|\", y)\n",
        "\n",
        "    print(\"ACCURACY\")\n",
        "    print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "0\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "0\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.2156524441211401\n",
            "DECISION PARAMETER INDEX\n",
            "3\n",
            "LIMIT VALUE\n",
            "0.2\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "1\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "2\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.3725087546767559\n",
            "DECISION PARAMETER INDEX\n",
            "1\n",
            "LIMIT VALUE\n",
            "3.0\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.23353054685632402\n",
            "DECISION PARAMETER INDEX\n",
            "0\n",
            "LIMIT VALUE\n",
            "5.5\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "1\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "2\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.09755145068629123\n",
            "DECISION PARAMETER INDEX\n",
            "3\n",
            "LIMIT VALUE\n",
            "1.9\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "2\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.02920935761790011\n",
            "DECISION PARAMETER INDEX\n",
            "3\n",
            "LIMIT VALUE\n",
            "2.2\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.16962312544795516\n",
            "DECISION PARAMETER INDEX\n",
            "0\n",
            "LIMIT VALUE\n",
            "6.5\n",
            "\n",
            "\n",
            "PARAMETERS\n",
            "[[6.3 2.5 4.9 1.5]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [4.9 3.6 1.4 0.1]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.  6.1 2.3]]\n",
            "\n",
            "\n",
            "PREDICTIONS|CLASS\n",
            "1 | 1\n",
            "2 | 2\n",
            "1 | 2\n",
            "1 | 1\n",
            "0 | 0\n",
            "1 | 2\n",
            "1 | 1\n",
            "0 | 0\n",
            "0 | 0\n",
            "0 | 1\n",
            "2 | 2\n",
            "0 | 0\n",
            "0 | 1\n",
            "1 | 2\n",
            "2 | 2\n",
            "ACCURACY\n",
            "0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "# depth = 3\n",
        "get_accuracy(3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# depth = 3\n",
        "PARAMETERS\n",
        "[[6.3 2.5 4.9 1.5]\n",
        " [6.8 3.  5.5 2.1]\n",
        " [6.4 2.8 5.6 2.2]\n",
        " [5.6 3.  4.1 1.3]\n",
        " [4.9 3.6 1.4 0.1]\n",
        " [6.  3.  4.8 1.8]\n",
        " [6.3 2.3 4.4 1.3]\n",
        " [4.4 3.2 1.3 0.2]\n",
        " [4.4 2.9 1.4 0.2]\n",
        " [5.5 2.6 4.4 1.2]\n",
        " [6.9 3.1 5.1 2.3]\n",
        " [5.5 4.2 1.4 0.2]\n",
        " [5.2 2.7 3.9 1.4]\n",
        " [6.5 3.  5.5 1.8]\n",
        " [7.7 3.  6.1 2.3]]\n",
        "\n",
        "\n",
        "PREDICTIONS|CLASS\n",
        "1 | 1\n",
        "2 | 2\n",
        "1 | 2\n",
        "1 | 1\n",
        "0 | 0\n",
        "1 | 2\n",
        "1 | 1\n",
        "0 | 0\n",
        "0 | 0\n",
        "0 | 1\n",
        "2 | 2\n",
        "0 | 0\n",
        "0 | 1\n",
        "1 | 2\n",
        "2 | 2\n",
        "ACCURACY\n",
        "0.6666666666666666\n",
        "\n",
        "# Dla głębokości drzewa 3 wartość accuracy przyjmuje ok. 67%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "0\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "0\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "1\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.33319765827861564\n",
            "DECISION PARAMETER INDEX\n",
            "0\n",
            "LIMIT VALUE\n",
            "4.8\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "0\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.4166666666666666\n",
            "DECISION PARAMETER INDEX\n",
            "1\n",
            "LIMIT VALUE\n",
            "3.0\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.2156524441211401\n",
            "DECISION PARAMETER INDEX\n",
            "3\n",
            "LIMIT VALUE\n",
            "0.2\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "1\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "2\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "2\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.0009641372096677314\n",
            "DECISION PARAMETER INDEX\n",
            "2\n",
            "LIMIT VALUE\n",
            "5.1\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.4904528182865035\n",
            "DECISION PARAMETER INDEX\n",
            "2\n",
            "LIMIT VALUE\n",
            "4.7\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "2\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "2\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.01818090322224869\n",
            "DECISION PARAMETER INDEX\n",
            "2\n",
            "LIMIT VALUE\n",
            "5.6\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "0\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.6979098932374422\n",
            "DECISION PARAMETER INDEX\n",
            "1\n",
            "LIMIT VALUE\n",
            "3.4\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.3725087546767559\n",
            "DECISION PARAMETER INDEX\n",
            "1\n",
            "LIMIT VALUE\n",
            "3.0\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.23353054685632402\n",
            "DECISION PARAMETER INDEX\n",
            "0\n",
            "LIMIT VALUE\n",
            "5.5\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "1\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "2\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "2\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.011676588555406292\n",
            "DECISION PARAMETER INDEX\n",
            "0\n",
            "LIMIT VALUE\n",
            "7.2\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.42902969900766774\n",
            "DECISION PARAMETER INDEX\n",
            "3\n",
            "LIMIT VALUE\n",
            "1.5\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "2\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.09755145068629123\n",
            "DECISION PARAMETER INDEX\n",
            "3\n",
            "LIMIT VALUE\n",
            "1.9\n",
            "\n",
            "\n",
            "NEW LEAF\n",
            "LEAF CLASS VALUE\n",
            "2\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.02920935761790011\n",
            "DECISION PARAMETER INDEX\n",
            "3\n",
            "LIMIT VALUE\n",
            "2.2\n",
            "\n",
            "\n",
            "NEW DECISION NODE\n",
            "INFORMATION GAIN\n",
            "0.16962312544795516\n",
            "DECISION PARAMETER INDEX\n",
            "0\n",
            "LIMIT VALUE\n",
            "6.5\n",
            "\n",
            "\n",
            "PARAMETERS\n",
            "[[6.3 2.5 4.9 1.5]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [4.9 3.6 1.4 0.1]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.  6.1 2.3]]\n",
            "\n",
            "\n",
            "PREDICTIONS|CLASS\n",
            "2 | 1\n",
            "2 | 2\n",
            "2 | 2\n",
            "1 | 1\n",
            "0 | 0\n",
            "2 | 2\n",
            "1 | 1\n",
            "0 | 0\n",
            "0 | 0\n",
            "1 | 1\n",
            "2 | 2\n",
            "0 | 0\n",
            "1 | 1\n",
            "2 | 2\n",
            "2 | 2\n",
            "ACCURACY\n",
            "0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "# depth = 5\n",
        "get_accuracy(5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# depth = 5\n",
        "PARAMETERS\n",
        "[[6.3 2.5 4.9 1.5]\n",
        " [6.8 3.  5.5 2.1]\n",
        " [6.4 2.8 5.6 2.2]\n",
        " [5.6 3.  4.1 1.3]\n",
        " [4.9 3.6 1.4 0.1]\n",
        " [6.  3.  4.8 1.8]\n",
        " [6.3 2.3 4.4 1.3]\n",
        " [4.4 3.2 1.3 0.2]\n",
        " [4.4 2.9 1.4 0.2]\n",
        " [5.5 2.6 4.4 1.2]\n",
        " [6.9 3.1 5.1 2.3]\n",
        " [5.5 4.2 1.4 0.2]\n",
        " [5.2 2.7 3.9 1.4]\n",
        " [6.5 3.  5.5 1.8]\n",
        " [7.7 3.  6.1 2.3]]\n",
        "\n",
        "\n",
        "PREDICTIONS|CLASS\n",
        "2 | 1\n",
        "2 | 2\n",
        "2 | 2\n",
        "1 | 1\n",
        "0 | 0\n",
        "2 | 2\n",
        "1 | 1\n",
        "0 | 0\n",
        "0 | 0\n",
        "1 | 1\n",
        "2 | 2\n",
        "0 | 0\n",
        "1 | 1\n",
        "2 | 2\n",
        "2 | 2\n",
        "ACCURACY\n",
        "0.9333333333333333"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "f9e27f2c6b8fafad1e5bf7a778dc6bdf3e9d4ad008263112f3fcfe55865c487e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
